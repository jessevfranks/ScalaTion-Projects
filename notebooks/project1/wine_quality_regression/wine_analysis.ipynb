{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab55dd37-f50c-40a5-8d62-cfe8659908d1",
   "metadata": {},
   "source": [
    "# Report: Wine Quality #\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b24e6-ae3d-452f-a197-7b0f7cd8cbc2",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##\n",
    "---\n",
    "\n",
    "**Purpose:** Make sure that the model isn't learning garbage. <br>\n",
    "**Check:** Missing values, outliers, scaling, skewed variables.\n",
    "\n",
    "**Missing Values:**<br>\n",
    "The wine dataset has no null/missing values. \n",
    "\n",
    "**Outliers:**<br>\n",
    "The points shown outside the whiskers of the boxplots represent values that fall more than 1.5 × IQR from the middle 50% of the data. The boxplot flags these as potential outliers, but in this dataset a lot of these values are not errors or anomalies. The wine chemistry variables (such as sulphates, residual sugar, and chlorides) are naturally skewed (their values are not evenly distributed around the center). Because of this skewness, the standard boxplot rule identifies a large number of observations as outliers, even though they are legitimate measurements. The observations are retained to preserve the true variability of the data. Removing them could distort the relationships between variables and negatively affect the regression analysis. The presence of skewed distributions motivates the use of transformed regression models (like log, square-root, Box-Cox, or Yeo-Johnson transformations), which help stabilize variance and improve model fit without discarding valid data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14539358-9de9-4d05-a653-1b3e4f7e2b3f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) ##\n",
    "---\n",
    "\n",
    "We use the EDA to justify our modeling choices later on.\n",
    "\n",
    "**Check:** \n",
    "* Variable relationships with target:\n",
    "    * Checking correlation strength revealed several predictors (particularly alcohol, volatile acidity, and sulphates) show clear relationships with wine quality. However, some relationships display curvature, suggesting violations of linearity assumptions.\n",
    "* Data shape diagnostic:\n",
    "    * QQ plots are used to check if each variable is normally distributed.\n",
    "    * We check normality because non-normal predictors can cause nonlinear relationships and violate regression assumptions.\n",
    "    * This detects skewness which then signals that transformation is necessary.\n",
    "    * Histograms indicated multiple predictors are right-skewed, motivating the use of log and Box–Cox transformations.\n",
    "* Predictor correlation:\n",
    "    * Correlation matrices are used to check collinearity/multicollinearity. |r|>0.7 is a warning.\n",
    "    * The correlation matrix revealed substantial multicollinearity among acidity-related variables, justifying the application of Ridge and Lasso regression to stabilize estimates and perform feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c751ddb-66cc-4d13-9e1d-33550e669dc8",
   "metadata": {},
   "source": [
    "### Quality of Fit\n",
    "**Using project1_utils.py**\n",
    "\n",
    "| Metric Type              | R² Value | What It Measures                                      | Verdict |\n",
    "|--------------------------|----------|--------------------------------------------------------|---------|\n",
    "| In-Sample R²             | 0.361    | Fit on the same data used to train the model           | Moderate fit; may be optimistic but not excessive |\n",
    "| Validation R² (80–20)    | 0.4032   | Predictive performance on unseen test data             | Consistent with training → model generalizes reasonably |\n",
    "| 5-Fold Cross-Validation  | 0.3424   | Average performance across multiple resampled splits   | Most reliable estimate; confirms only moderate explanatory power |\n",
    "\n",
    "Because all of these values are close together, the model generalizes fairly well.\n",
    "These show that the model explains (roughly) 30% - 40% of the variability in wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07053454",
   "metadata": {},
   "source": [
    "### Feature Selection: \n",
    "\n",
    "Features (updated ranking):\n",
    "* fixed acidity\n",
    "* alcohol\n",
    "* sulphates\n",
    "* volatile acidity\n",
    "* density\n",
    "* total sulfur dioxide\n",
    "* chlorides\n",
    "* free sulfur dioxide\n",
    "* pH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27257c-a7d1-48af-81f5-a1ea236bcfb4",
   "metadata": {},
   "source": [
    "## Evaluate Models ##\n",
    "---\n",
    "**Using WineQualityRegression.scala**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e7e10-5615-490e-be26-d8e487b4244f",
   "metadata": {},
   "source": [
    "### Linear Regression \n",
    "\n",
    "| Metric | In-Sample (Full) | Train-Test Split (80/20) | 5-Fold Cross-Validation (Avg) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **$R^2$** | 0.352067 | 0.352067 | 0.3381522 |\n",
    "| **Adj. $R^2$** | 0.347987 | 0.347987 | 0.270089 |\n",
    "| **RMSE** | 0.649844 | 0.649844 | 0.655629 |\n",
    "| **MAPE** | 9.29% | 9.29% | 9.39% |\n",
    "\n",
    "Linear Regression (LR) creates a baseline model that links wine chemistry to quality, but its moderate R² suggests that the relationship is only partially captured by a linear type structure. Also, some predictors add little explanatory value, motivating the use of regularization and feature-selection methods (Ridge, Lasso, and Stepwise) to test whether a simpler or more stable model can perform as well or even better.\n",
    "The R² indicates that approximately 35% of the variation in wine quality is explained by the measured chemical properties. 35% is a meaningful relationship, but it also means that wine quality is definitely influenced by other unmeasured factors.\n",
    "Five-fold cross-validation had similar performance across folds, meaning the model generalizes fairly well and is not severely overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8a148-e7a3-4d2d-bbcc-b2c8431038a1",
   "metadata": {},
   "source": [
    "### Ridge Regression \n",
    "\n",
    "| Model  | R²  | RMSE |\n",
    "| ------ | --- | ---- |\n",
    "| Linear | 0.352067 | 0.649844  |\n",
    "| Ridge  | 0.351946 | 0.649905  |\n",
    "\n",
    "With Ridge Regression (RR) we test for overfitting risk and regularization. Our results show that RR produced comparable performance to LR, suggesting limited multicollinearity impact.\n",
    "RR was applied to address potential multicollinearity by shrinking coefficient magnitudes using L2 regularization (λ = 0.1). However, predictive performance remained about the same to the baseline linear regression (R² = 0.352, RMSE = 0.650), this indicates that multicollinearity was not substantially devaluing model generalization. So, regularization provided little benefit for this dataset (the added bias was not super useful).\n",
    "Since ridge regression did not improve RMSE or R², multicollinearity was not severely harming the linear model. While we saw a couple predictor pairs that had a strong correlation, the vast majority of variables are fairly distinct, so these RR results make sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e847f1-41f9-4fe5-8c36-39572b1d2840",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "| Model  | R²       | RMSE     | # Predictors |\n",
    "| ------ | -------- | -------- | ------------ |\n",
    "| Linear | 0.352067 | 0.649844 | 11           |\n",
    "| Ridge  | 0.351946 | 0.649905 | 11           |\n",
    "| Lasso  | 0.351005 | 0.650376 | 8            |\n",
    "\n",
    "Lasso Regression (LR) removes any coefficient that is exactly 0.00000. Those predictors don't add a useful signal.\n",
    "Lasso regression applied L1 regularization (λ = 0.1), which shrinks some coefficients to zero, performing automatic **feature selection**. The method removed residual sugar, free sulfur dioxide, and total sulfur dioxide from the model, reducing the predictor set without affecting predictive accuracy (R² = 0.351, RMSE = 0.650). This shows that these variables don't give much information for predicting wine quality and that a simpler model can achieve comparable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6ffd2-3ea8-4f70-b1af-d39c4fe45e3f",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "| Model  | Question Answered                       |\n",
    "| ------ | --------------------------------------- |\n",
    "| Linear | What relationships exist?               |\n",
    "| Ridge  | Does multicollinearity hurt prediction? |\n",
    "| Lasso  | Which variables actually matter?        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6e9b1-2f27-4c9e-a03c-2a082f0147a7",
   "metadata": {},
   "source": [
    "### Transformed Regression Comparison\n",
    "\n",
    "| Model | R² | RMSE | MAPE | Verdict |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Baseline (OLS)** | **0.352067** | **0.649844** | **9.29%** | **Best** |\n",
    "| Sqrt Transform | 0.350789 | 0.650485 | 9.25% | Worse |\n",
    "| Box-Cox | 0.351008 | 0.650375 | 9.25% | Worse |\n",
    "| Yeo-Johnson | 0.351205 | 0.650276 | 9.26% | Woese |\n",
    "| Log1p | 0.348866 | 0.651447 | 9.23% | Worse |\n",
    "\n",
    "\n",
    "| Observation                                                       | Meaning                                                    |\n",
    "| ----------------------------------------------------------------- | ---------------------------------------------------------- |\n",
    "| All transformed models have very similar (R^2) (~0.349-0.351)     | Transformations did **not improve explanatory power**      |\n",
    "| RMSE values differ by only ~0.001                                 | Prediction accuracy is basically unchanged                 |\n",
    "| MAPE values remain between 9.2–9.3%                               | Relative prediction error stayed the same                  |\n",
    "| OLS already had the best (R²)                                    | The original scale already fit the linear assumptions well |\n",
    "\n",
    "Different transformation strategies (square-root, log1p, Box–Cox, and Yeo–Johnson) were tested to face possible nonlinearity and skewness in the predictors. All transformed models produced nearly identical performance to the original linear regression, with R² values around 0.35 and RMSE near 0.65. This says that the original variables already satisfied the assumptions of linear regression pretty well and that transformation did not improve predictive accuracy in a meaningful way. The baseline OLS model is the best for its interpretability and comparable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cb01d-a6e9-41ab-8f1c-f9b78a4959c6",
   "metadata": {},
   "source": [
    "### Symbolic Ridge Regression\n",
    "\n",
    "The Symbolic Ridge Regression demonstrated that increasing a model's complexity without sufficient data or scaling can severely degrade performance. SymRidge regression was evaluated to capture potential nonlinear relationships through automatic feature expansion. However, the model exhibited terrible numerical instability, producing extremely large coefficient estimates, undefined standard errors (NaN), and a super negative R². These results say that the expanded feature space introduced extreme multicollinearity and badly conditioned matrix calculations (prob at the inversion step) that the regularization parameter couldn't control. SymRidge underperformed all simpler models which shows that increased model complexity is not supported by this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19627a93-e0df-4158-91c4-899f730f88c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda]",
   "language": "python",
   "name": "conda-env-.conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
