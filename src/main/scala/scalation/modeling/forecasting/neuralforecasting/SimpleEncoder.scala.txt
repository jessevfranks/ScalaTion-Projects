
//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author  John A. Miller
 *  @version 2.0
 *  @date    Sun Nov  9 23:27:14 EST 2025
 *  @see     LICENSE (MIT style license file).
 *
 *  @note    Simple Implementation for an Encoder-Only Transformer, e.g., used for
 *           (1) Natural Language Processing (NLP) or
 *           (2) Time Series Forecasting (TSF) 
 *
 *  Limitations: one attention head, no dropout layer, single encoder block,
 *               no back-propagation
 *
 *  @see sebastianraschka.com/blog/2023/self-attention-from-scratch.html
 *  @see arxiv.org/pdf/1706.03762.pdf (main paper)
 */

package scalation
package modeling
package forecasting
package neuralforecasting

import scala.math.{cos, sin, sqrt}

import scalation.mathstat._
import scalation.modeling.ActivationFun.{f_reLU, f_softmax}

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `SimpleEncoder` object implements the attention method based on the
 *  scaled dot product.
 */
object SimpleEncoder:

    private val debug = debugf ("SimpleEncoder", true)        // debug function
            val eps   = 1E-5                                  // very small value

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Patchify the univariate time series y by breaking it into non-overlapping
     *  patches of length pl.  This simple implementation assumes stride s = pl,
     *  but PatchTST uses pl = 16 and s = 8 as defaults.
     *  @param y   the given univariate time series
     *  @param pl  the patch length
     */
    def patchify (y: VectorD, pl: Int): MatrixD =
        val m  = y.dim
        val np = m / pl
        val x  = new MatrixD (np, pl)
        for i <- x.indices do x(i) = y(i*pl until (i+1)*pl)
        x
    end patchify

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Use a matrix transformation containing learnable weights to embed each patch
     *  vector into a higher dimensional space (providing enhanced vector similarity).
     *  The dimensionality of the embedding space is d_model.  For this simple
     *  implementation d_model = d_k as there is only one attention head.
     *  @param xx  the matrix containing each patch as a row
     *  @param wE  the dimensionality of the embedding space
     */
    def embed (xx: MatrixD, wE: MatrixD): MatrixD = xx * wE

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Encode all the positions in the time series as vectors of length d_model.
     *  @param len  the sequence length
     *  @param d_k  the dimensionality of the model (d_model = d_k here)
     */
    def encodePositions (len: Int, d_k: Int): MatrixD =
        val pe = new MatrixD (len, d_k)
        for k <- pe.indices do
            for i <- 0 until d_k/2 do
                val den = 10000.0~^(2.0*i/d_k)
                pe(k, 2*i)   = sin (k / den)      
                pe(k, 2*i+1) = cos (k / den)      
        pe
    end encodePositions

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Based on the Query (Q), Key (K), and Value (V) matrices, compute the attention.
     *
     *      att = softmax (QK^T/âˆšd_k) V
     *
     *  @param q    the Query: the input of interest
     *  @param k    the Key: other locations to compare it with (for similarity)
     *  @param v    the Value: the input value at the key locations
     *  @param d_k  the dimensionality of Query, Key, and Value (if different use d_v) 
     */
    def attention (q: MatrixD, k: MatrixD, v: MatrixD, d_k: Int): MatrixD =
        val qkt = q * k.ð“                                     // repeated dot product
        val sdp = qkt / sqrt (d_k)                            // scaled dot product (sdp)
        val scr = f_softmax.fM (sdp)                          // attention scores
        debug ("attention", s" qkt = $qkt, sdp = $sdp, scr = $scr")
        scr * v                                               // attention (Q, K, V)
    end attention

    //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    /** Perform layer normalization on matrix x.  While batch normalization normalizes
     *  over a mini-batch, layer normalization normalizes over a single instance or row.
     *  @see www.geeksforgeeks.org/deep-learning/what-is-layer-normalization/
     *  An affine transformation is supported via the Î³ and Î² learnable parameters.
     *  @param x  the matrix to normalize
     *  @param Î³  the scaling learnable parameter (defaults to 1.0)
     *  @param Î²  the shifting learnable parameter (defaults to 0.0)
     */
    def layerNorm (x: MatrixD, Î³: Double = 1.0, Î²: Double = 0.0): MatrixD = 
        val xt = x.ð“                                          // transpose since ScalaTion computes means and stdevs column-wise
        val xn = ((xt - xt.mean) / (xt.stdev + eps)).ð“        // z-transform to mean zero and stdev one
        xn * Î³ + Î²                                            // allows scaling and shifting for flexibility
    end layerNorm

end SimpleEncoder

import SimpleEncoder._

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleEncoder1` main function illustrates the calculation of attention (Q, K, V)
 *  for a Single Head as used in a Transformer.  SEE LINK BELOW FOR MORE DETAILS.
 *
 *  @see pub.aimind.so/transformer-model-and-variants-of-transformer-chatgpt-3d423676e29c (URL)
 *
 *  > runMain scalation.modeling.forecasting.neuralforecasting.simpleEncoder1
 */
@main def simpleEncoder1 (): Unit =
 
    val d_k = 3                                               // dimensionality for Q, K, and V (if different need d_v)
//  val heads = 1                                             // number of attention heads (d_model = d_k * heads)

    // input token can be (sub) words (for NLP) or patches (for TSF)
    // three inputs after embedding, each embedding vector has size/dimensionality 4
    // these three embedding vectors are made up, but could use word2vec, etc.
    val x = MatrixD ((3, 4), 1, 0, 1, 0,                      // input x0 for token 0
                             0, 2, 0, 2,                      // input x1 for token 1
                             1, 1, 1, 1)                      // input x2 for token 2

    println (s"input (after embedding) x = $x")

    val wQ = MatrixD ((4, 3), 1, 0, 1,                        // Query weight matrix
                              1, 0, 0,
                              0, 0, 1,
                              0, 1, 1) 
    val wK = MatrixD ((4, 3), 0, 0, 1,                        // Key weight matrix
                              1, 1, 0,
                              0, 1, 0,
                              1, 1, 0) 
    val wV = MatrixD ((4, 3), 0, 2, 0,                        // Value weight matrix
                              0, 3, 0,
                              1, 0, 3,
                              1, 1, 0)

    val q = x * wQ                                            // Query: size of input x d_k
    val k = x * wK                                            // Key:   size of input x d_k
    val v = x * wV                                            // Value: size of input x d_k (or d_v if different)

//  val att = attention (q, k, v, d_k)                        // compute attention based on correct size
    val att = attention (q, k, v, 1)                          // approximation used by URL for checking purposes

    println (s"""
    d_k = $d_k
    wQ  = $wQ
    wK  = $wK
    wV  = $wV
    q   = $q
    k   = $k
    v   = $v
    att = $att
    """)

end simpleEncoder1


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleEncoder2` main function illustrates the steps in an "Encoder-Only Transformer"
 *  consisting of a single encoder block with a "Prediction Head" added for making forecasts.
 *  > runMain scalation.modeling.forecasting.neuralforecasting.simpleEncoder2
 */
@main def simpleEncoder2 (): Unit =

    import neuralnet.SimpleCNN.yy                             // example univariate time series of length 20 (t = 0 ... 19)
    println (s"|| time series yy = \n $yy")
    val y_h1 = 6.5                                            // next actual value y(20) to compare with forecast
                                                              // rolling validation skipped for simplicity

    val pl  = 4                                               // patch length
    val d_k = 6                                               // dimensionality for Q, K, and V (if different need d_v)
//  val heads = 1                                             // number of attention heads (d_model = d_k * heads)

    // input tokens in this case are patches (for TSF)
    // five inputs with each patch vector having size/dimensionality 4
    val (Î¼, Ïƒ) = (yy.mean, yy.stdev)
    val y  = (yy - Î¼) / (Ïƒ + eps)                             // normalize the whole time series via z-transformation 
    val xx = patchify (y, pl)                                 // patchify to form a 5 by 4 matrix
    println (s"|| input after normalize and patchify y -> xx = $xx")

    //-----------------------------------------
    // Input Embedding + Positional Encodings |
    //-----------------------------------------

    val wE = MatrixD.fill (xx.dim2, d_k, 0.1)                 // transformation matrix holding learnable embedding weights
                                                              // improve initialization: use random Normal and rescale
    var x  = embed (xx, wE)                                   // embed xx in a higher dimensional space
    println (s"|| input after (higher dimensional) embedding xx -> x = $x")

    val pe = encodePositions (xx.dim, d_k)                    // create positional encodings
    x     += pe                                               // add positional encodings
    println (s"|| positional encodings pe = $pe")
    println (s"|| input (after adding positional encoding) x = $x")

    //------------------
    // Attention Layer |
    //------------------

    val wQ = MatrixD.fill (d_k, d_k, 0.1)                     // Query weight matrix: improve initialization: randomize
    val wK = MatrixD.fill (d_k, d_k, 0.1)                     // Key weight matrix
    val wV = MatrixD.fill (d_k, d_k, 0.1)                     // Value weight matrix

    val q = x * wQ                                            // Query: size of input x d_k
    val k = x * wK                                            // Key:   size of input x d_k
    val v = x * wV                                            // Value: size of input x d_k (or d_v if different)

    val att = attention (q, k, v, d_k)                        // compute attention

    println (s"""
    wQ  = $wQ
    wK  = $wK
    wV  = $wV
    q   = $q
    k   = $k
    v   = $v
    att = $att
    """)

    //-----------------------------
    // Add & Norm After Attention |
    //-----------------------------

    x += att                                                  // add attention to x (residual connection)
    x  = layerNorm (x)                                        // apply layer normalization (use default Î³ and Î²)
    println (s"|| after layer normalization x = $x")

    //--------------------------------------------------------
    // Two-Layer (Hidden, Output) Feed Forward Network (FFN) |
    //--------------------------------------------------------

    val d_ff = 2 * d_k                                        // dimensionality of FFN hidden layer (commonly use four-fold expansion)
    val w1 = MatrixD.fill (d_k, d_ff, 0.1)                    // weight matrix preceding the FFN hidden layer: improve initialization: randomize
    val b1 = VectorD.fill (d_ff)(0.1)                         // bias vector for the FFN hidden layer
    val w2 = MatrixD.fill (d_ff, d_k, 0.1)                    // weight matrix preceding the FFN output layer
    val b2 = VectorD.fill (d_k)(0.1)                          // bias vector for the FFN output layer

    // FFN forward prop: 'refined input' -> hidden            // input with embedding, position encoding, attention, layer norm 
    val u  = x * w1 + b1                                      // hidden pre-activation matrix
    val z  = f_reLU.fM (u)                                    // hidden matrix from f0 = reLU activation
//  val z  = f_geLU.fM (u)                                    // hidden matrix from f0 = geLU activation

    // FFN forward prop: hidden -> output
    val vv = z * w2 + b2                                      // output pre-activation matrix
    val Å·  = vv                                               // output/prediction matrix: typically no activation

    println (s"""
    u  = $u
    z  = $z
    vv = $vv
    Å·  = $Å·
    """)

    //-----------------------
    // Add & Norm After FNN |
    //-----------------------

    x += Å·                                                    // add output from FNN to x (residual connection)
    x  = layerNorm (x)                                        // apply layer normalization a second time (use default Î³ and Î²)
    println (s"|| at end of encoder block x = $x")

    //-------------------------------------------------------
    // Prediction Head/Additional Linear Layer: Horizon = 1 |
    //-------------------------------------------------------

    val h_last = x(x.dim-1)                                   // use the last patch for prediction (simple example)
    println (s"h_last.dim = ${h_last.dim}")                  

    val w_out = VectorD.fill (d_k)(0.1)                       // weight vector for the prediction head, improve initialization: randomize
    val b_out = 0.0                                           // bias scalar for the prediction head

    val Å·_norm = h_last âˆ™ w_out + b_out                       // scalar on normalized (z) scale
    val Å·_h1   = Î¼ + (Ïƒ + eps) * Å·_norm                       // apply back-transformation to get forecast

    println (s"forecast (normalized)     = $Å·_norm")
    println (s"forecast (original scale) = $Å·_h1")

    val Îµ = y_h1 - Å·_h1                                       // error at horizon 1 (actual - forecasted)

    println (s"forecast error at time t = ${y.dim}: Îµ = $Îµ")

    // extensions: (1) use more than the last patch from the encoder by flattening matrix x
    // (2) add a hidden layer (possibly more than one) before the ending Linear Layer
    // (3) perform multi-horizon forecasting
    // (4) perform rolling validation

    // No backward prop -- should use AutoDiff due to complexity

end simpleEncoder2

